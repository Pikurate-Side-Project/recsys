{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jerry/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jerry/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "mecab = Mecab(os.environ['MECAB_DIC_PATH'])\n",
    "stemmer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/link_data.csv').fillna('')\n",
    "\n",
    "\n",
    "\n",
    "unique_pik = dict(set(zip(df.pik_id, df.pik_title)))\n",
    "unique_category = dict(set(zip(df.category_id, df.category_title)))\n",
    "unique_link = dict(set(zip(df.link_id, df.memo)))\n",
    "\n",
    "index_to_pik = {idx: pik_id for idx, pik_id in enumerate(df.pik_id)}\n",
    "\n",
    "pik_to_index = {x: idx for idx, x in enumerate(list(unique_pik.keys()))}\n",
    "category_to_index = {x: idx for idx, x in enumerate(list(unique_category.keys()))}\n",
    "link_to_index = {x: idx for idx, x in enumerate(list(unique_link.keys()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pik_to_num = df.groupby(['pik_id'], as_index=False).count()\n",
    "pik_to_num = {row[0]: row[1] for _, row in pik_to_num.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text: str):\n",
    "    \n",
    "    result = re.sub(pattern=r'[\\[\\]():|]', repl='', string=text)\n",
    "    result = re.sub(pattern=r'\\s', repl=' ', string=result)\n",
    "    result = re.sub(pattern=r'[一-龥]', repl='', string=result)\n",
    "    result = re.sub(pattern=r'[ㄱ-ㅎㅏ-ㅣ]', repl=' ', string=result)\n",
    "\n",
    "    result.strip()\n",
    "    result = ' '.join(result.split())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_noun(text: str, target_pos=['NNG', 'NNP', 'NNB']) -> List[str]:\n",
    "\n",
    "    def is_noun(word: Tuple[str, str]) -> bool:\n",
    "\n",
    "        if word[1] in target_pos:\n",
    "            return True\n",
    "        elif word[1] == 'SL' and nltk.pos_tag([word[0]])[0][1] == 'NN':\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    pos_list : List[Tuple[str, str]] = mecab.pos(preprocess(text))\n",
    "    result = [stemmer.lemmatize(p[0]) for p in pos_list if is_noun(p)]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_pik = TfidfVectorizer(tokenizer=lambda x: extract_noun(x), min_df=2, max_features=500)\n",
    "tfidf_vectorizer_category = TfidfVectorizer(tokenizer=lambda x: extract_noun(x), min_df=2, max_features=500)\n",
    "tfidf_vectorizer_link = TfidfVectorizer(tokenizer=lambda x: extract_noun(x), min_df=2, max_features=500)\n",
    "# count_vectorizer = CountVectorizer(tokenizer=lambda x: extract_noun(mecab, x), min_df=2, max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_pik = tfidf_vectorizer_pik.fit_transform(list(unique_pik.values()))\n",
    "vec_category = tfidf_vectorizer_category.fit_transform(list(unique_category.values()))\n",
    "vec_link = tfidf_vectorizer_link.fit_transform(list(unique_link.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4014x2 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = CountVectorizer(tokenizer=lambda x: extract_noun(x), min_df=2, vocabulary=['sns', '디저트'])\n",
    "s.fit_transform(list(unique_pik.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_matrix(\n",
    "    pik_matrix, category_matrix, link_matrix,\n",
    "    pik_to_index, category_to_index, link_to_index\n",
    "):\n",
    "    pik_matrix_index = [pik_to_index[i] for i in df.pik_id]\n",
    "    category_matrix_index = [category_to_index[i] for i in df.category_id]\n",
    "    link_matrix_index = [link_to_index[i] for i in df.link_id]\n",
    "\n",
    "    tmp_pik_vec = pik_matrix[pik_matrix_index]\n",
    "    tmp_category_vec = category_matrix[category_matrix_index]\n",
    "    tmp_link_vec = link_matrix[link_matrix_index]\n",
    "\n",
    "    concat_matrix = np.concatenate((tmp_pik_vec, tmp_category_vec, tmp_link_vec), axis=1)\n",
    "\n",
    "    return concat_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50442, 1500)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_matrix = concat_matrix(vec_pik.toarray(), vec_category.toarray(), vec_link.toarray(), pik_to_index, category_to_index, link_to_index)\n",
    "concat_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=20)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Kmeans = KMeans(n_clusters=20)\n",
    "Kmeans.fit(concat_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = Kmeans.predict(concat_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_by_label(target_label: int, labels: List[int]) -> pd.DataFrame:\n",
    "    target_ids = [idx for idx, label in enumerate(labels) if label == target_label]\n",
    "    return df.loc[target_ids]\n",
    "\n",
    "def extract_statics_by_label(target_label: int, labels: List[int]) -> Dict[int, int]:\n",
    "    target_ids = [idx for idx, label in enumerate(labels) if label == target_label]\n",
    "    target_df = df.loc[target_ids]\n",
    "    grouped = target_df.groupby(['pik_id']).count()\n",
    "\n",
    "    return {row[0]: row[1][0] for row in grouped.iterrows()}\n",
    "\n",
    "def convert_ratio(target_dict: Dict[int, int]) -> Dict[int, Dict[str, float]]:\n",
    "    result = {pik_id: {'title': unique_pik[pik_id], 'ratio': num / pik_to_num[pik_id]} for pik_id, num in target_dict.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from matplotlib import colors as mcolors\n",
    "\n",
    "\n",
    "colors = mcolors.TABLEAU_COLORS\n",
    "by_hsv = {name: mcolors.to_rgba(np.append(mcolors.rgb_to_hsv(mcolors.to_rgba(color)[:3]), .4))\n",
    "                for name, color in colors.items()}\n",
    "\n",
    "color_names = list(by_hsv.keys())\n",
    "random.shuffle(color_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerry/anaconda3/envs/tokenizer/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "/home/jerry/anaconda3/envs/tokenizer/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "tsne = TSNE(random_state = 42)\n",
    "# TSNE에는 transform 메서드가 없으므로 대신 fit_transform을 사용한다.\n",
    "link_tsne = tsne.fit_transform(concat_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.xlim(link_tsne[:,0].min(), link_tsne[:,0].max()+1)\n",
    "plt.ylim(link_tsne[:,1].min(), link_tsne[:,1].max()+1)\n",
    "\n",
    "tsne_point_by_label = dict()\n",
    "\n",
    "for i in range(len(concat_matrix)):\n",
    "    label = labels[i]\n",
    "    try:\n",
    "        tsne_point_by_label[label]['x'].append(link_tsne[i, 0])\n",
    "        tsne_point_by_label[label]['y'].append(link_tsne[i, 1])\n",
    "    except KeyError:\n",
    "        tmp_dict = {'x': [link_tsne[i, 0]], 'y': [link_tsne[i, 1]]}\n",
    "        tsne_point_by_label[label] = tmp_dict\n",
    "\n",
    "\n",
    "for label, coordinates in tsne_point_by_label.items():\n",
    "    plt.plot(coordinates['x'], coordinates['y'], 'ro', color=by_hsv[color_names[label % 10]], label=f'group {label}')\n",
    "    \n",
    "plt.xlabel(\"t-SNE x\")\n",
    "plt.ylabel(\"t-SNE y\")\n",
    "\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.legend(loc=(1.0, .6), frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.50482087,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.47876988, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.34710283,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.62885206,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.50482087,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.47876988, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.34710283,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.62885206,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = df.loc[df.pik_id == 6436, :]\n",
    "tfidf_vectorizer_pik.transform(sample.pik_title).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_noun(text: str, target_pos=['NNG', 'NNP', 'NNB']) -> List[str]:\n",
    "\n",
    "    def is_noun(word: Tuple[str, str]) -> bool:\n",
    "\n",
    "        if word[1] in target_pos:\n",
    "            return True\n",
    "        elif word[1] == 'SL' and nltk.pos_tag([word[0]])[0][1] == 'NN':\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    pos_list : List[Tuple[str, str]] = mecab.pos(preprocess(text))\n",
    "    result = [stemmer.lemmatize(p[0]) for p in pos_list if is_noun(p)]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KmeansModel:\n",
    "\n",
    "    def __init__(self, tokenizer, config):\n",
    "        self._model = None\n",
    "        self._labels= None\n",
    "        \n",
    "        self._index_to_pik = None\n",
    "        self._pik_to_title = None\n",
    "        self._pik_to_num = None\n",
    "\n",
    "        self._tfidf_vectorizer_pik = None\n",
    "        self._tfidf_vectorizer_category = None\n",
    "        self._tfidf_vectorizer_link = None\n",
    "\n",
    "        self.tokenzier = tokenizer\n",
    "        self.n_cluster = config.n_cluster\n",
    "        self.min_df = config.min_df\n",
    "        self.max_feature = config.max_feature\n",
    "    \n",
    "    def train(self, training_data):\n",
    "        \n",
    "        unique_pik = dict(set(zip(training_data.pik_id, training_data.pik_title)))\n",
    "        unique_category = dict(set(zip(training_data.category_id, training_data.category_title)))\n",
    "        unique_link = dict(set(zip(training_data.link_id, training_data.memo)))\n",
    "\n",
    "        index_to_pik = {idx: pik_id for idx, pik_id in enumerate(training_data.pik_id)}\n",
    "\n",
    "        pik_to_index = {x: idx for idx, x in enumerate(list(unique_pik.keys()))}\n",
    "        category_to_index = {x: idx for idx, x in enumerate(list(unique_category.keys()))}\n",
    "        link_to_index = {x: idx for idx, x in enumerate(list(unique_link.keys()))}\n",
    "\n",
    "        pik_to_num = training_data.groupby(['pik_id'], as_index=False).count()\n",
    "        pik_to_num = {row[0]: row[1] for _, row in pik_to_num.iterrows()}\n",
    "\n",
    "        tfidf_vectorizer_pik = TfidfVectorizer(tokenizer=lambda x: self.tokenzier(x), \\\n",
    "            min_df=self.min_df, max_features=self.max_feature)\n",
    "        tfidf_vectorizer_category = TfidfVectorizer(tokenizer=lambda x: self.tokenzier(x), \\\n",
    "            min_df=self.min_df, max_features=self.max_feature)\n",
    "        tfidf_vectorizer_link = TfidfVectorizer(tokenizer=lambda x: self.tokenzier(x), \\\n",
    "            min_df=self.min_df, max_features=self.max_feature)\n",
    "        \n",
    "        vec_pik = tfidf_vectorizer_pik.fit_transform(list(unique_pik.values())).toarray()\n",
    "        vec_category = tfidf_vectorizer_category.fit_transform(list(unique_category.values())).toarray()\n",
    "        vec_link = tfidf_vectorizer_link.fit_transform(list(unique_link.values())).toarray()\n",
    "\n",
    "        concated_matrix = KmeansModel.concat_matrix(vec_pik, vec_category, vec_link, pik_to_index, category_to_index, link_to_index)\n",
    "\n",
    "        Kmeans = KMeans(n_clusters=self.n_cluster)\n",
    "        Kmeans.fit(concated_matrix)\n",
    "        \n",
    "        # allocate\n",
    "        self._model = Kmeans\n",
    "        self._labels= self._model.predict(concated_matrix)\n",
    "        \n",
    "        self._index_to_pik = index_to_pik\n",
    "        self._pik_to_title = unique_pik\n",
    "        self._pik_to_num = pik_to_num\n",
    "\n",
    "        self._tfidf_vectorizer_pik = tfidf_vectorizer_pik\n",
    "        self._tfidf_vectorizer_category = tfidf_vectorizer_category\n",
    "        self._tfidf_vectorizer_link = tfidf_vectorizer_link\n",
    "        \n",
    "\n",
    "    def predict(self, pik_id=0):\n",
    "\n",
    "        assert self._model, f'The model is not trained yet, you must train first!'\n",
    "\n",
    "        assert self._tfidf_vectorizer_pik, f'The model is not trained yet, you must train first!'\n",
    "        assert self._tfidf_vectorizer_category, f'The model is not trained yet, you must train first!'\n",
    "        assert self._tfidf_vectorizer_link, f'The model is not trained yet, you must train first!'\n",
    "\n",
    "        # make concat_matrix\n",
    "        unique_pik = dict(set(zip(sample.pik_id, sample.pik_title)))\n",
    "        unique_category = dict(set(zip(sample.category_id, sample.category_title)))\n",
    "        unique_link = dict(set(zip(sample.link_id, sample.memo)))\n",
    "\n",
    "        pik_to_index = {x: idx for idx, x in enumerate(list(unique_pik.keys()))}\n",
    "        category_to_index = {x: idx for idx, x in enumerate(list(unique_category.keys()))}\n",
    "        link_to_index = {x: idx for idx, x in enumerate(list(unique_link.keys()))}\n",
    "\n",
    "        # extract label\n",
    "        vec_pik = self._tfidf_vectorizer_pik.fit_transform(list(unique_pik.values())).toarray()\n",
    "        vec_category = self._tfidf_vectorizer_category.transform(list(unique_category.values())).toarray()\n",
    "        vec_link = self._tfidf_vectorizer_link.transform(list(unique_link.values())).toarray()\n",
    "\n",
    "        pik_matrix = concat_matrix(vec_pik, vec_category, vec_link, pik_to_index, category_to_index, link_to_index)\n",
    "        \n",
    "        # analysis\n",
    "        labels = self._model.predict(pik_matrix)\n",
    "        link_num = len(labels)\n",
    "        \n",
    "        from collections import Counter\n",
    "        label_freq = dict(Counter(labels))\n",
    "        label_freq = {label: num / link_num for label, num in label_freq.items()}\n",
    "\n",
    "        # return result\n",
    "        return label_freq\n",
    "    \n",
    "    def save(self, path):\n",
    "        import pickle\n",
    "\n",
    "        with open(path, 'wb') as m:\n",
    "            pickle.dump(Kmeans, m)\n",
    "\n",
    "    def extract_statics_by_label(self, target_label, desc=True):\n",
    "        \n",
    "        assert self._index_to_pik, f'The model is not trained yet, you must train first!'\n",
    "        assert self._pik_to_title, f'The model is not trained yet, you must train first!'\n",
    "        assert self._pik_to_num, f'The model is not trained yet, you must train first!'\n",
    "\n",
    "        from collections import Counter\n",
    "        \n",
    "        target_piks = [self._index_to_pik[idx] for idx, label in enumerate(self._labels) \n",
    "            if label == target_label]\n",
    "        pik_freq = dict(Counter(target_piks))\n",
    "        result = {pik_id: {'title': self._pik_to_title[pik_id], 'ratio': num / self._pik_to_num[pik_id]} for pik_id, num in pik_freq.items()}\n",
    "        \n",
    "        if desc:\n",
    "            result = sorted(result.items(), key=lambda x: x[1].get('ratio'))\n",
    "            result.reverse()\n",
    "        \n",
    "    @staticmethod\n",
    "    def concat_matrix(\n",
    "        pik_matrix, category_matrix, link_matrix,\n",
    "        pik_to_index, category_to_index, link_to_index\n",
    "    ):\n",
    "        pik_matrix_index = [pik_to_index[i] for i in df.pik_id]\n",
    "        category_matrix_index = [category_to_index[i] for i in df.category_id]\n",
    "        link_matrix_index = [link_to_index[i] for i in df.link_id]\n",
    "\n",
    "        tmp_pik_vec = pik_matrix[pik_matrix_index]\n",
    "        tmp_category_vec = category_matrix[category_matrix_index]\n",
    "        tmp_link_vec = link_matrix[link_matrix_index]\n",
    "\n",
    "        concat_matrix = np.concatenate((tmp_pik_vec, tmp_category_vec, tmp_link_vec), axis=1)\n",
    "\n",
    "        return concat_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "sample_model = KmeansModel(Kmeans, labels, index_to_pik, unique_pik, pik_to_num)\n",
    "\n",
    "with open('./models/c20.d1500.pkl', 'wb') as m:\n",
    "    saved_model = pickle.dump(Kmeans, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./models/c20.d1500.pkl', 'rb') as m:\n",
    "    loaded_model = pickle.load(m)\n",
    "\n",
    "loaded_model.predict(concat_matrix[0].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goose'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.lemmatize(\"geese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f70198ab6b25b51d4ad1ef0b5d1caadbcf98f69e8c063257b8d65c25d4dd6e33"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('tokenizer': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
